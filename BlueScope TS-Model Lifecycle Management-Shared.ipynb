{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Using the DataRobot API to streamline model building and deployment workflow__\n",
    "\n",
    "## Steps:\n",
    "- Step 1 - Importing the libraries \n",
    "- Setp 2 - Get our API Token \n",
    "- Step 3 - Load the dataset \n",
    "- Step 4 - Setup the project \n",
    "- Step 5 - Run the Automated Time Series Process across multiple stores\n",
    "- Step 6 - Deploy the best model \n",
    "- Step 7 - Make predictions \n",
    "\n",
    "But before we begin, let me walk through the problem that we are trying to solve first...\n",
    "\n",
    "# Step 1 - Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datarobot \n",
    "import datarobot as dr\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2 - Get our API Token \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<datarobot.rest.RESTClientObject at 0x7ff85b835590>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = \"yourapitoken\"\n",
    "endpoint = \"https://app.datarobot.com/api/v2\"\n",
    "dr.Client(token = token, endpoint = endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 - Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The main dataset\n",
    "filename = '2frag_trend_daily_2020.csv'\n",
    "project_name = 'BlueScope-NorthStar-TS-Project-2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the calendar\n",
    "#calendar = dr.CalendarFile.create('event_calendar.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the full list of projects in DataRobot  \n",
    "```\n",
    "dr.Project.list()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4 - Setup the project \n",
    "## Create project in DataRobot \n",
    "\n",
    "If the project already exist we can just pick it up using this command \n",
    "```\n",
    "id = '###'\n",
    "proj = dr.Project.get(id)\n",
    "```\n",
    "\n",
    "or delete an existing project ... \n",
    "```\n",
    "id = '###'\n",
    "proj = dr.Project.get(id)\n",
    "proj.delete()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Known-In-Advance Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#known_in_advance = ['Marketing', 'TouristEvent']\n",
    "#feature_settings = [dr.FeatureSettings(feat_name, known_in_advance=True) for feat_name in known_in_advance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_settings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the partition specification \n",
    "time_partition = dr.DatetimePartitioningSpecification(\n",
    "    datetime_partition_column='Date',\n",
    "    use_time_series=True,\n",
    "    feature_settings=None,\n",
    "    calendar_id=None,\n",
    "    gap_duration='P1D',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5 - Run the Automated Time Series Process across multiple stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets read the data in pandas\n",
    "raw_data = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.214951445"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data['#2Frag Cu_calc'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingDataSet = pd.read_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start time\n",
    "start = datetime.datetime.now()\n",
    "project = dr.Project.start(sourcedata=TrainingDataSet,\n",
    "                                project_name = project_name, \n",
    "                                target = '#2Frag Cu_calc',\n",
    "                                partitioning_method=time_partition,\n",
    "                                worker_count = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Whilst we wait for this to finish, lets check out a few things: \n",
    " - Check out the public documentation and examples [here](https://datarobot-public-api-client.readthedocs-hosted.com/en/v2.21.3/examples/index.html)\n",
    "\n",
    "- Check out our community blog [here](https://community.datarobot.com/t5/guided-ai-learning/introduction-to-a-model-factory/ta-p/1599) to see other plots such as ROC curves and Feature effects\n",
    "\n",
    "- Check out the DataRobot main console to see what's happening \n",
    "\n",
    "- Now lets have a look at a custom model deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In progress: 1, queued: 0 (waited: 0s)\n",
      "In progress: 1, queued: 0 (waited: 2s)\n",
      "In progress: 1, queued: 0 (waited: 4s)\n",
      "In progress: 1, queued: 0 (waited: 5s)\n",
      "In progress: 0, queued: 0 (waited: 7s)\n",
      "In progress: 0, queued: 0 (waited: 10s)\n",
      "In progress: 0, queued: 0 (waited: 15s)\n",
      "In progress: 0, queued: 0 (waited: 22s)\n",
      "In progress: 0, queued: 0 (waited: 36s)\n",
      "In progress: 0, queued: 0 (waited: 57s)\n",
      "In progress: 1, queued: 0 (waited: 79s)\n",
      "In progress: 1, queued: 0 (waited: 100s)\n",
      "In progress: 1, queued: 0 (waited: 121s)\n",
      "In progress: 1, queued: 0 (waited: 142s)\n",
      "In progress: 0, queued: 0 (waited: 164s)\n",
      "In progress: 1, queued: 0 (waited: 186s)\n",
      "In progress: 0, queued: 0 (waited: 208s)\n",
      "In progress: 1, queued: 0 (waited: 229s)\n",
      "In progress: 1, queued: 0 (waited: 250s)\n",
      "In progress: 1, queued: 0 (waited: 271s)\n",
      "In progress: 1, queued: 0 (waited: 293s)\n",
      "In progress: 1, queued: 0 (waited: 315s)\n",
      "In progress: 0, queued: 0 (waited: 336s)\n",
      "In progress: 0, queued: 0 (waited: 357s)\n",
      "In progress: 0, queued: 0 (waited: 378s)\n",
      "13.683333333333334 minutes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Wait for AutoPilot to finish\n",
    "\n",
    "project.wait_for_autopilot()\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "delta = end - start \n",
    "print(delta.seconds / 60, 'minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 - Deploy the best model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Model('Ridge Regressor with Forecast Distance Modeling')\n",
      "None\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "# quick check to see which models we are deploying for each series \n",
    "\n",
    "best_models = project.get_models()[0]\n",
    "print('--------------------------------')\n",
    "#print('Best model for admission type id: %s' %key)\n",
    "print(best_models)\n",
    "print(best_models.metrics['RMSE']['crossValidation'])\n",
    "print('--------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get the model ID \n",
    "\n",
    "model = project.get_models()[0]\n",
    "\n",
    "# Create the deployment \n",
    "deployment = dr.Deployment.create_from_learning_model(\n",
    "    model.id,\n",
    "    label='DEMO_DEPLOYMENT_BLUESCOPE'  ,\n",
    "    default_prediction_server_id=dr.PredictionServer.list()[0].id\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets have a look at the deployments page whilst we wait for the deployments to be created "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7 - Make predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment(DEMO_DEPLOYMENT_BLUESCOPE)\n"
     ]
    }
   ],
   "source": [
    "pred_input = pd.read_csv(\"2frag_trend_daily_2020_predictions.csv\")\n",
    "print(deployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "601b5739534c868107822b5c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BatchPredictionJob(batchPredictions, '601b60b393557bf44ee0a956', status=INITIALIZING)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "deployment_id = deployment.id\n",
    "# Check that we are sending the scoring data to the right deployment\n",
    "print(deployment_id)\n",
    "\n",
    "dr.BatchPredictionJob.score_to_file(\n",
    "    deployment_id,\n",
    "    pred_input,\n",
    "    'Output_2frag_trend_daily_2020_predictions.csv',\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is only a small teaser.\n",
    "\n",
    "We can of course, easily integrate with other systems seamlessly which is the biggest strength of DataRobot. See this example where a TS model is trained periodically and predictions wrote back to a proper DB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
